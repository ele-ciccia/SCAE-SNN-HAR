PREPROCESSING OF THE DATA:

- The real data are measurements obtained in a lab of the department. The experiment included 7 people for which different activities were recorded multiple times. The activities are: WALKING, SITTING, RUNNING and moving HANDS. Each measurement is the CIR of shape 110 (# of range bins) x sample length, which varies from experiment to experiment. 
- From the original matrix we extracted the FFT with windows of length 64 and stride 32, obtaining a matrix of shape (110, # of windows). Then the absolute value was taken and the top 10 range bins were kept, the other were discarded. 
- Each CIR is normalized in the range [0,1] with the min-max normalization --> (X - min) / (max - min).
- The CIR is complex: we extracted real and imaginary part to feed them to a real-valued neural network.
- The CIR has size [2, 232, 10, 64], where 2 is given by the fact that we are considering both the real and complex part; 232 is the number of windows (we cropped the CIR with more windows, in order to have the same number of windows, and discarded CIR windows with fewer windows. In this way the neural network will process elements of the same size); 10 is the number of range bins kept; 64 is the length of each window.
- We do not need lot of windows: the information useful to classify is contained in a number of windows that corresponds roughly to 2-3 seconds. For this reason, we considered chunck of 2 seconds, which roughly corresponds to 232 windows (we chose a sampling time T=0.27 ms)
- The sequences with more than 232 windows were divided in non-overlapping sequences of 232 windows, obtaining in these way more data from the same recorded sequence.

- The dataset was divided in train-valid-test with the following proportion: 0.8-0.1-0.1. The training set contains 935 elements, while the valid and test have 88 and 89 elements, respectively.
- To see the distribution of the activities in each set, see Real_dataset.ipynb
- Each element of the dataset is (X, md_columns, Y) where X is the CIR signal, md_columns is the extracted mu_D spectrum and Y is the label of the activity.




NETWORK STRUCTURE
- Conv3D to process the incoming data of dimension 4 (232 x 2 x 10 x 64). The kernel has shape 1 x 1 x kerne_size, so that we just convolve on the 64 dimension, which is the one we are interested in.
- # of timesteps in SNN == # windows
- The optimization and first train of the network was done with the CAE as encoding part and using rate coding as output decoding.

HYPERPARAMETER OPTIMIZATION:
- The hyperparameters (N. of conv layers in the CAE, n. of channels in the CAE, kernel size, threshold tau, alpha and beta, n. of SNN layers, n. of SNN LIF neurons, optimization algorithm and learning rate) were first optimized one by one, to find two or three good values, and then with these values a Grid Search was performed, choosing the best combination of hyperparams as the one giving the best mean accuracy on the validation set. 
- For Grid Search I used Ray library with Basic Variant Generator as search algorithm.

- Once obtained the best configuration of hyperparams (see params.py for the values), I run the sparsity regularization with lambda values in [0.0, 1.0] with step 0.1. 
 [TO DO] Do multiple runs to average over more trials.
 
 - Finally, I trained the network for 150 epochs with patience 20, saving the best model in terms of accuracy in the valid set.
 
 
 NEXT STEPS:
 1) Vedere capacità di encoding dei neuroni LIF ed eventualmente sostituire CAE con SNN. In questo caso, comparare anche # parametri (grandezza/peso del modello in MB), velocità di inferenza, accuratezza della ricostruzione tra CAE e SNN
 2) Provare a cambiare tipo di regolarizzazione (usare quella con correlazione tra real e imag?)
 3) Vedere differenza tra Spike rate e spike timing come output decoding sia in termini di: accuratezza della classificazione, numero di spike prodotte, tempi di inferenza
 4) provare a incorporare poi POPULATION CODING
 5) Scrivere funzioni per calcolare precision-recall-F1, visto che il nostro dataset è altamente sbilanciato nelle classi. Plottare anche confusion matrix
 6) Plottare spike and membrane potential dei vari layer per vedere cosa succede
